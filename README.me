# GIH

This code accompanies the paper:\
\[[ICLR'2025]((https://iclr.cc/))\] \[[Geometric Inductive Biases of Deep Networks: The Role of Data and Architecture](https://arxiv.org/abs/2410.12025)\]\
Sajad Movahedi, Antonio Orvieto, Seyed-Mohsen Moosavi-Dezfooli

The code is based on the implementation of [Neural Anisotropy Directions](https://github.com/LTS4/neural-anisotropy-directions).

## Abstract
In this paper, we propose the *geometric invariance hypothesis (GIH)*, which argues that the input space curvature of a neural network remains invariant under transformation in certain architecture-dependent directions during training. We investigate a simple, non-linear binary classification problem residing on a plane in a high dimensional space and observe that—unlike MLPs—ResNets fail to generalize depending on the orientation of the plane. Motivated by this example, we define a neural network's **average geometry** and **average geometry evolution** as compact architecture-dependent summaries of the model's input-output geometry and its evolution during training. By investigating the average geometry evolution at initialization, we discover that the geometry of a neural network evolves according to the data covariance projected onto its average geometry. This means that the geometry only changes in a subset of the input space when the average geometry is low-rank, such as in ResNets. This causes an architecture-dependent invariance property in the input space curvature, which we dub GIH. Finally, we present extensive experimental results to observe the consequences of GIH and how it relates to generalization in neural networks.

![Geometric Invariance Hypothesis in
ResNet.](geometry_evolution_illustration_v3.pdf)

## Requirements
```
Python >= 3.11.10
PyTorch >= 2.1.1
torchvision == 0.16.1
```
## Experiments
For performing the experiments in figures 2 and 8 of the paper, run ```covar_exp.py``` with the appropriate settings.

For performing the experiments in figure 3, run ```inv_g_exp.py``` with the appropriate settings.

For performing the experiments in figure 4, run ```cifar_acc_exp.py``` with the appropriate settings.

For performing the experiments in figure 10, run ```normal_acc_exp.py``` with the appropriate settings.

For performing the experiments in figure 5, run ```sbh_exp.py``` with the appropriate settings.

For performing the experiments in figure 6, run ```orth_proj_exp.py``` with the appropriate settings.

For performing the experiments in tables 1 and 2, run ```pruning_exp.py``` with the appropriate settings.

The function ```compute_G``` performs the computation of $\mathbf{G}_{\mathcal{F}}$ in the paper.

## Citations
If you find any part of this code useful in your research, please cite the following papers:

```
@InCollection{OrtizModasNADs2020,
    TITLE = {{Neural Anisotropy Directions}},
    AUTHOR = {{Ortiz-Jimenez}, Guillermo and {Modas}, Apostolos and {Moosavi-Dezfooli}, Seyed-Mohsen and Frossard, Pascal},
    BOOKTITLE = {Advances in Neural Information Processing Systems 34},
    MONTH = dec,
    YEAR = {2020}
}
```

```
@inproceedings{
    movahedi2025geometric,
    title={Geometric Inductive Biases of Deep Networks: The Role of Data and Architecture},
    author={Sajad Movahedi and Antonio Orvieto and Seyed-Mohsen Moosavi-Dezfooli},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=cmXWYolrlo}
}
```